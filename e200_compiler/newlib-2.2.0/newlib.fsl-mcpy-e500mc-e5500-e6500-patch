diff -ruN newlib-2.2.0-orig/newlib/libc/include/powerpc/asm.h newlib-2.2.0-memcpy-fixed/newlib/libc/include/powerpc/asm.h
--- newlib-2.2.0-orig/newlib/libc/include/powerpc/asm.h	2015-09-23 14:29:36.308121000 -0500
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/include/powerpc/asm.h	2015-09-23 14:29:04.116121001 -0500
@@ -29,4 +29,15 @@
 #define r1 1
 #define r2 2
 
+#ifdef __ASSEMBLER__
+#define C_SYMBOL_NAME(name) name
+#define ASM_LINE_SEP ;
+#define weak_alias(original,alias) .weak C_SYMBOL_NAME (alias) \
+  ASM_LINE_SEP C_SYMBOL_NAME (alias) = C_SYMBOL_NAME (original)
+#else
+#define weak_alias(name,aliasname) _weak_alias (name, aliasname)
+#define _weak_alias(name,aliasname) \
+  extern __typeof (name) aliasname __attribute__ ((weak, alias (#name)));
+#endif
+
 #endif
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/configure newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/configure
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/configure	2015-09-23 14:29:36.315121001 -0500
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/configure	2015-09-23 14:29:04.118121002 -0500
@@ -557,6 +557,14 @@
 ac_subst_vars='LTLIBOBJS
 LIBOBJS
 extra_objs
+TARGET_64_FALSE
+TARGET_64_TRUE
+E6500_FALSE
+E6500_TRUE
+E5500_FALSE
+E5500_TRUE
+E500MC_FALSE
+E500MC_TRUE
 sys_dir
 machine_dir
 libm_machine_dir
@@ -3402,6 +3410,82 @@
 
 
 
+{ $as_echo "$as_me:${as_lineno-$LINENO}: checking whether the compiler supports any of e500mc/e5500/e6500" >&5
+$as_echo_n "checking whether the compiler supports any of e500mc/e5500/e6500... " >&6; }
+if ${compiler_cv_cpu_select+:} false; then :
+  $as_echo_n "(cached) " >&6
+else
+
+cat > conftest.c <<EOF
+int
+main ()
+{
+
+  ;
+  return 0;
+}
+EOF
+if { ac_try='${CC-cc} -S -v conftest.c 2>&1'
+  { { eval echo "\"\$as_me\":${as_lineno-$LINENO}: \"$ac_try\""; } >&5
+  (eval $ac_try) 2>&5
+  ac_status=$?
+  $as_echo "$as_me:${as_lineno-$LINENO}: \$? = $ac_status" >&5
+  test $ac_status = 0; }; } | egrep 'mcpu=e500mc|mcpu=e5500|mcpu=e6500' > conftest.err 2>&1;
+then
+  if (grep 'mcpu=e500mc' conftest.err) >/dev/null 2>&1; then
+    compiler_cv_cpu_select=e500mc
+  elif (grep 'mcpu=e5500' conftest.err) >/dev/null 2>&1; then
+    compiler_cv_cpu_select=e5500
+  elif (grep 'mcpu=e6500' conftest.err) >/dev/null 2>&1; then
+    compiler_cv_cpu_select=e6500
+  fi
+  if (grep 'm64' conftest.err) >/dev/null 2>&1; then
+    compiler_cpu_64_bit=yes
+  else
+    compiler_cpu_64_bit=no
+  fi
+else
+  compiler_cpu_select=no
+  compiler_cpu_64_bit=no
+fi
+rm -f conftest*
+fi
+{ $as_echo "$as_me:${as_lineno-$LINENO}: result: $compiler_cv_cpu_select" >&5
+$as_echo "$compiler_cv_cpu_select" >&6; }
+
+ if test $compiler_cv_cpu_select = e500mc; then
+  E500MC_TRUE=
+  E500MC_FALSE='#'
+else
+  E500MC_TRUE='#'
+  E500MC_FALSE=
+fi
+
+ if test $compiler_cv_cpu_select = e5500; then
+  E5500_TRUE=
+  E5500_FALSE='#'
+else
+  E5500_TRUE='#'
+  E5500_FALSE=
+fi
+
+ if test $compiler_cv_cpu_select = e6500; then
+  E6500_TRUE=
+  E6500_FALSE='#'
+else
+  E6500_TRUE='#'
+  E6500_FALSE=
+fi
+
+ if test $compiler_cpu_64_bit = yes; then
+  TARGET_64_TRUE=
+  TARGET_64_FALSE='#'
+else
+  TARGET_64_TRUE='#'
+  TARGET_64_FALSE=
+fi
+
+
 extra_objs=
 case $host in
   powerpc*-*altivec*)
@@ -3590,8 +3674,24 @@
   as_fn_error $? "conditional \"USE_LIBTOOL\" was never defined.
 Usually this means the macro was only invoked conditionally." "$LINENO" 5
 fi
+if test -z "${E500MC_TRUE}" && test -z "${E500MC_FALSE}"; then
+  as_fn_error $? "conditional \"E500MC\" was never defined.
+Usually this means the macro was only invoked conditionally." "$LINENO" 5
+fi
+if test -z "${E5500_TRUE}" && test -z "${E5500_FALSE}"; then
+  as_fn_error $? "conditional \"E5500\" was never defined.
+Usually this means the macro was only invoked conditionally." "$LINENO" 5
+fi
+if test -z "${E6500_TRUE}" && test -z "${E6500_FALSE}"; then
+  as_fn_error $? "conditional \"E6500\" was never defined.
+Usually this means the macro was only invoked conditionally." "$LINENO" 5
+fi
+if test -z "${TARGET_64_TRUE}" && test -z "${TARGET_64_FALSE}"; then
+  as_fn_error $? "conditional \"TARGET_64\" was never defined.
+Usually this means the macro was only invoked conditionally." "$LINENO" 5
+fi
 
-: ${CONFIG_STATUS=./config.status}
+: "${CONFIG_STATUS=./config.status}"
 ac_write_fail=0
 ac_clean_files_save=$ac_clean_files
 ac_clean_files="$ac_clean_files $CONFIG_STATUS"
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/e500mc/memcpy.S newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e500mc/memcpy.S
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/e500mc/memcpy.S	1969-12-31 18:00:00.000000000 -0600
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e500mc/memcpy.S	2015-09-23 14:29:04.118121002 -0500
@@ -0,0 +1,268 @@
+/* Optimized memcpy implementation for e500mc PowerPC.  */
+
+#include <powerpc/asm.h>
+
+/* _PTR [r3] memcpy 
+   (_PTR __restrict dst [r3], const _PTR __restrict src [r4], size_t len [r5]);
+   Returns 'dst'.
+
+	 r3 = destination
+	 r4 = source
+	 r5 = byte count
+
+	 volatile fixed point registers usable:
+	 r0, r3-r12
+
+	 volatile floating point registers usable:
+	 f0-f13.  */
+
+FUNC_START (memcpy)
+	cmplw	cr0, r4, r3		/* if source==destination, return.  */
+	beqlr	cr0
+	/* if number of bytes is less than 8, (optimal value TBD),
+	   but greater than zero copy byte-by-byte.  */
+	cmplwi	r5, 8
+	mr	r6, r3
+	blt	L(copy_bytes)
+	neg	r0, r4
+	andi.	r11, r0, 7
+	beq	L(src_aligned)
+	/* We have to align the src pointer by r11 bytes */
+	cmplwi	cr1, r11, 4
+	cmplwi	cr0, r11, 1
+	bgt	cr1, L(src_567)
+	ble	cr0, L(src_1)
+	/* 2, 3 or 4 bytes */
+	addi	r0, r11, -2
+	lhz	r9, 0(r4)
+	lhzx	r12, r4, r0
+	sth	r9, 0(r6)
+	sthx	r12, r6, r0
+	b	L(src_0)
+L(src_567):
+	addi	r0, r11, -4
+	lwz	r9, 0(r4)
+	lwzx	r12, r4, r0
+	stw	r9, 0(r6)
+	stwx	r12, r6, r0
+	b	L(src_0)
+L(src_1):
+	lbz	r0, 0(r4)
+	stb	r0, 0(r6)
+L(src_0):
+	subf	r5, r11, r5
+	add	r4, r4, r11
+	add	r6, r6, r11
+L(src_aligned):
+	cmplwi	7, r5, 63
+	ble	7, L(copy_remaining)
+	srwi	r11, r5, 6		/* No of 64 byte copy count.  */
+	rlwinm	r5, r5, 0, 26, 31	/* remaining bytes.  */
+	rlwinm.	r0, r6, 0, 29, 31
+	mtctr	r11
+	bne	0, L(dst_naligned)
+L(copy_dalign):
+#ifndef _SOFT_FLOAT
+	lfd	0, 0(r4)
+	lfd	1, 8(r4)
+	lfd	2, 16(r4)
+	lfd	3, 24(r4)
+	stfd	0, 0(r6)
+	stfd	1, 8(r6)
+	stfd	2, 16(r6)
+	stfd	3, 24(r6)
+	lfd	0, 32(r4)
+	lfd	1, 40(r4)
+	lfd	2, 48(r4)
+	lfd	3, 56(r4)
+	addi	r4, r4, 64
+	stfd	0, 32(r6)
+	stfd	1, 40(r6)
+	stfd	2, 48(r6)
+	stfd	3, 56(r6)
+#else
+	lwz	r0, 0(r4)
+	lwz	r8, 4(r4)
+	lwz	r9, 8(r4)
+	stw	r0, 0(r6)
+	stw	r8, 4(r6)
+	stw	r9, 8(r6)
+	lwz	r0, 12(r4)
+	lwz	r8, 16(r4)
+	lwz	r9, 20(r4)
+	stw	r0, 12(r6)
+	stw	r8, 16(r6)
+	stw	r9, 20(r6)
+	lwz	r0, 24(r4)
+	lwz	r8, 28(r4)
+	lwz	r9, 32(r4)
+	stw	r0, 24(r6)
+	stw	r8, 28(r6)
+	stw	r9, 32(r6)
+	lwz	r0, 36(r4)
+	lwz	r8, 40(r4)
+	lwz	r9, 44(r4)
+	stw	r0, 36(r6)
+	stw	r8, 40(r6)
+	stw	r9, 44(r6)
+	lwz	r0, 48(r4)
+	lwz	r8, 52(r4)
+	lwz	r9, 56(r4)
+	stw	r0, 48(r6)
+	lwz	r0, 60(r4)
+	addi	r4, r4, 64
+	stw	r8, 52(r6)
+	stw	r9, 56(r6)
+	stw	r0, 60(r6)
+#endif
+	addi	r6, r6, 64
+	bdnz	L(copy_dalign)
+L(copy_remaining):
+	srwi.	r11, r5, 3		/* No of 8 byte copy count.  */
+	rlwinm	r5, r5, 0, 29, 31	/* remaining bytes.  */
+	beq	0, L(copy_bytes)
+	mtcrf	0x01, r11
+	bf	cr7*4+1, L(cp16b)
+	lwz	r0, 0(r4)		/* copy 32 bytes.  */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	lwz	r0, 16(r4)
+	lwz	r7, 20(r4)
+	lwz	r8, 24(r4)
+	lwz	r9, 28(r4)
+	addi	r4, r4, 32
+	stw	r0, 16(r6)
+	stw	r7, 20(r6)
+	stw	r8, 24(r6)
+	stw	r9, 28(r6)
+	addi	r6, r6, 32
+L(cp16b):
+	bf	cr7*4+2, L(cp8b)
+	lwz	r0, 0(r4)		/* copy 16 bytes.  */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	addi	r4, r4, 16
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	addi	r6, r6, 16
+L(cp8b):
+	bf	cr7*4+3, L(copy_bytes)
+	lwz	r0, 0(r4)		/* copy 8 bytes.  */
+	lwz	r7, 4(r4)
+	addi	r4, r4, 8
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	addi	r6, r6, 8
+L(copy_bytes):
+	cmplwi	cr1, r5, 4
+	cmplwi	cr0, r5, 1
+	bgt	cr1, L(gt4b)		/* nb > 4?  (5, 6, 7 bytes).  */
+	ble	cr0, L(lt1b)		/* nb <= 1? (0, 1 bytes).  */
+	addi	r0, r5, -2		/* 2, 3, 4 bytes.  */
+	lhz	r9, 0(r4)
+	lhzx	r11, r4, r0
+	sth	r9, 0(r6)
+	sthx	r11, r6, r0
+	blr
+L(gt4b):
+	addi	r0, r5, -4		/* 5, 6, 7 bytes.  */
+	lwz	r9, 0(r4)
+	lwzx	r11, r4, r0
+	stw	r9, 0(r6)
+	stwx	r11, r6, r0
+	blr
+L(lt1b):
+	mtocrf	0x1, r5			/* nb == 0 ? return.  */
+	bflr	31
+	lbz	r0, 0(r4)		/* nb == 1.  */
+	stb	r0, 0(r6)
+	blr
+
+L(dst_naligned):
+	rlwinm.	r0, r6, 0, 30, 31
+	beq	0, L(copy_dalign4)
+L(copy_dnalign):
+	lwz	r0, 0(r4)		/* copy 64 bytes.  */
+	lwz	r8, 4(r4)
+	lwz	r9, 8(r4)
+	stw	r0, 0(r6)
+	stw	r8, 4(r6)
+	stw	r9, 8(r6)
+	lwz	r0, 12(r4)
+	lwz	r8, 16(r4)
+	lwz	r9, 20(r4)
+	stw	r0, 12(r6)
+	stw	r8, 16(r6)
+	stw	r9, 20(r6)
+	lwz	r0, 24(r4)
+	lwz	r8, 28(r4)
+	lwz	r9, 32(r4)
+	stw	r0, 24(r6)
+	stw	r8, 28(r6)
+	stw	r9, 32(r6)
+	lwz	r0, 36(r4)
+	lwz	r8, 40(r4)
+	lwz	r9, 44(r4)
+	stw	r0, 36(r6)
+	stw	r8, 40(r6)
+	stw	r9, 44(r6)
+	lwz	r0, 48(r4)
+	lwz	r8, 52(r4)
+	lwz	r9, 56(r4)
+	stw	r0, 48(r6)
+	lwz	r0, 60(r4)
+	addi	r4, r4, 64
+	stw	r8, 52(r6)
+	stw	r9, 56(r6)
+	stw	r0, 60(r6)
+	addi	r6, r6, 64
+	bdnz	L(copy_dnalign)
+	b	L(copy_remaining)
+
+L(copy_dalign4):
+	lwz	r0, 0(r4)
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	lwz	r0, 16(r4)
+	lwz	r7, 20(r4)
+	lwz	r8, 24(r4)
+	lwz	r9, 28(r4)
+	stw	r0, 16(r6)
+	stw	r7, 20(r6)
+	stw	r8, 24(r6)
+	stw	r9, 28(r6)
+	lwz	r0, 32(r4)
+	lwz	r7, 36(r4)
+	lwz	r8, 40(r4)
+	lwz	r9, 44(r4)
+	stw	r0, 32(r6)
+	stw	r7, 36(r6)
+	stw	r8, 40(r6)
+	stw	r9, 44(r6)
+	lwz	r0, 48(r4)
+	lwz	r7, 52(r4)
+	lwz	r8, 56(r4)
+	lwz	r9, 60(r4)
+	addi	r4, r4, 64
+	stw	r0, 48(r6)
+	stw	r7, 52(r6)
+	stw	r8, 56(r6)
+	stw	r9, 60(r6)
+	addi	r6, r6, 64
+	bdnz	L(copy_dalign4)
+	b	L(copy_remaining)
+
+FUNC_END (memcpy)
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/e5500/32-bit/memcpy.S newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e5500/32-bit/memcpy.S
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/e5500/32-bit/memcpy.S	1969-12-31 18:00:00.000000000 -0600
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e5500/32-bit/memcpy.S	2015-09-23 14:29:04.119121001 -0500
@@ -0,0 +1,235 @@
+/* Optimized memcpy implementation for e5500 32-bit PowerPC.  */
+
+#include <powerpc/asm.h>
+
+/* _PTR [r3] memcpy 
+   (_PTR __restrict dst [r3], const _PTR __restrict src [r4], size_t len [r5]);
+   Returns 'dst'.
+
+	 r3 = destination
+	 r4 = source
+	 r5 = byte count
+
+	 volatile fixed point registers usable:
+	 r0, r3-r12
+
+	 volatile floating point registers usable:
+	 f0-f13.  */
+
+FUNC_START (memcpy)
+	cmplw	cr0, r4, r3		/* if source==destination, return.  */
+	beqlr	cr0
+	/* if number of bytes is less than 8, (optimal value TBD),
+	   but greater than zero copy byte-by-byte.  */
+	cmplwi	r5, 8
+	mr	r6, r3
+	blt	L(copy_bytes)
+	neg	r0, r4
+	andi.	r11, r0, 3
+	beq	L(src_align4)
+	/* We have to align the src pointer by r11 bytes */
+	cmplwi	cr0, r11, 1
+	ble	L(src_1)
+	/* 2 or 3 bytes */
+	addi	r8, r11, -2
+	lhz	r9, 0(r4)
+	lhzx	r12, r4, r8
+	sth	r9, 0(r6)
+	sthx	r12, r6, r8
+	b	L(src_0)
+L(src_1):
+	lbz	r8, 0(r4)
+	stb	r8, 0(r6)
+L(src_0):
+	subf	r5, r11, r5
+	add	r4, r4, r11
+	add	r6, r6, r11
+L(src_align4):
+	/* Aligned by 4, now extend it to 16 */
+	cmplwi	7, r5, 63
+	ble	7, L(copy_remaining)
+	andi.	r10, r0, 15
+	beq	L(src_aligned16)
+	subf.	r10, r11, r10
+	beq	0, L(src_aligned16)
+	srwi	r11, r10, 2
+	subf	r5, r10, r5
+	mtctr	r11
+L(copy_salign16):
+	lwz	0, 0(r4)
+	addi	r4, r4, 4
+	stw	0, 0(r6)
+	addi	r6, r6, 4
+	bdnz	L(copy_salign16)
+L(src_aligned16):
+	srwi.	r11, r5, 6		/* No of 64 byte copy count.  */
+	beq	0, L(copy_remaining)
+	rlwinm	r5, r5, 0, 26, 31	/* remaining bytes.  */
+	rlwinm.	r0, r6, 0, 29, 31
+	mtctr	r11
+	bne	0, L(copy_dnalign)
+L(copy_dalign):
+#ifndef _SOFT_FLOAT
+	lfd	0, 0(r4)		/* copy 64 bytes.  */
+	lfd	1, 8(r4)
+	lfd	2, 16(r4)
+	lfd	3, 24(r4)
+	stfd	0, 0(r6)
+	stfd	1, 8(r6)
+	stfd	2, 16(r6)
+	stfd	3, 24(r6)
+	lfd	0, 32(r4)
+	lfd	1, 40(r4)
+	lfd	2, 48(r4)
+	lfd	3, 56(r4)
+	addi	r4, r4, 64
+	stfd	0, 32(r6)
+	stfd	1, 40(r6)
+	stfd	2, 48(r6)
+	stfd	3, 56(r6)
+	addi	r6, r6, 64
+#else
+	lwz	r0, 0(r4)
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	lwz	r0, 16(r4)
+	lwz	r7, 20(r4)
+	lwz	r8, 24(r4)
+	lwz	r9, 28(r4)
+	stw	r0, 16(r6)
+	stw	r7, 20(r6)
+	stw	r8, 24(r6)
+	stw	r9, 28(r6)
+	lwz	r0, 32(r4)
+	lwz	r7, 36(r4)
+	lwz	r8, 40(r4)
+	lwz	r9, 44(r4)
+	stw	r0, 32(r6)
+	stw	r7, 36(r6)
+	stw	r8, 40(r6)
+	stw	r9, 44(r6)
+	lwz	r0, 48(r4)
+	lwz	r7, 52(r4)
+	lwz	r8, 56(r4)
+	lwz	r9, 60(r4)
+	addi	r4, r4, 64
+	stw	r0, 48(r6)
+	stw	r7, 52(r6)
+	stw	r8, 56(r6)
+	stw	r9, 60(r6)
+	addi	r6, r6, 64
+#endif
+	bdnz	L(copy_dalign)
+L(copy_remaining):
+	srwi.	r11, r5, 3		/* No of 8 byte copy count.  */
+	rlwinm	r5, r5, 0, 29, 31	/* remaining bytes.  */
+	beq	0, L(copy_bytes)
+	mtcrf	0x01, r11
+	bf	cr7*4+1, L(cp16b)
+	lwz	r0, 0(r4)		/* copy 32 bytes.  */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	lwz	r0, 16(r4)
+	lwz	r7, 20(r4)
+	lwz	r8, 24(r4)
+	lwz	r9, 28(r4)
+	addi	r4, r4, 32
+	stw	r0, 16(r6)
+	stw	r7, 20(r6)
+	stw	r8, 24(r6)
+	stw	r9, 28(r6)
+	addi	r6, r6, 32
+L(cp16b):
+	bf	cr7*4+2, L(cp8b)
+	lwz	r0, 0(r4)		/* copy 16 bytes.  */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	addi	r4, r4, 16
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	addi	r6, r6, 16
+L(cp8b):
+	bf	cr7*4+3, L(copy_bytes)
+	lwz	r0, 0(r4)		/* copy 8 bytes.  */
+	lwz	r7, 4(r4)
+	addi	r4, r4, 8
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	addi	r6, r6, 8
+L(copy_bytes):
+	cmplwi	cr1, r5, 4
+	cmplwi	cr0, r5, 1
+	bgt	cr1, L(gt4b)		/* nb > 4?  (5, 6, 7 bytes).  */
+	ble	cr0, L(lt1b)		/* nb <= 1? (0, 1 bytes).  */
+	addi	r0, r5, -2		/* 2, 3, 4 bytes.  */
+	lhz	r9, 0(r4)
+	lhzx	r11, r4, r0
+	sth	r9, 0(r6)
+	sthx	r11, r6, r0
+	blr
+L(gt4b):
+	addi	r0, r5, -4		/* 5, 6, 7 bytes.  */
+	lwz	r9, 0(r4)
+	lwzx	r11, r4, r0
+	stw	r9, 0(r6)
+	stwx	r11, r6, r0
+	blr
+L(lt1b):
+	mtocrf	0x1, r5			/* nb == 0 ? return.  */
+	bflr	31
+	lbz	r0, 0(r4)		/* nb == 1.  */
+	stb	r0, 0(r6)
+	blr
+
+L(copy_dnalign):
+	lwz	r0, 0(r4)		/* copy 64 bytes.  */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	lwz	r0, 16(r4)
+	lwz	r7, 20(r4)
+	lwz	r8, 24(r4)
+	lwz	r9, 28(r4)
+	stw	r0, 16(r6)
+	stw	r7, 20(r6)
+	stw	r8, 24(r6)
+	stw	r9, 28(r6)
+	lwz	r0, 32(r4)
+	lwz	r7, 36(r4)
+	lwz	r8, 40(r4)
+	lwz	r9, 44(r4)
+	stw	r0, 32(r6)
+	stw	r7, 36(r6)
+	stw	r8, 40(r6)
+	stw	r9, 44(r6)
+	lwz	r0, 48(r4)
+	lwz	r7, 52(r4)
+	lwz	r8, 56(r4)
+	lwz	r9, 60(r4)
+	addi	r4, r4, 64
+	stw	r0, 48(r6)
+	stw	r7, 52(r6)
+	stw	r8, 56(r6)
+	stw	r9, 60(r6)
+	addi	r6, r6, 64
+	bdnz	L(copy_dnalign)
+	b	L(copy_remaining)
+
+FUNC_END (memcpy)
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/e5500/64-bit/memcpy.S newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e5500/64-bit/memcpy.S
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/e5500/64-bit/memcpy.S	1969-12-31 18:00:00.000000000 -0600
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e5500/64-bit/memcpy.S	2015-09-23 14:29:04.119121001 -0500
@@ -0,0 +1,137 @@
+/* Optimized memcpy implementation for e5500 64-bit PowerPC.  */
+
+#include <powerpc/asm.h>
+
+/* _PTR [r3] memcpy 
+   (_PTR __restrict dst [r3], const _PTR __restrict src [r4], size_t len [r5]);
+   Returns 'dst'.
+
+	 r3 = destination
+	 r4 = source
+	 r5 = byte count
+
+	 volatile fixed point registers usable:
+	 r0, r3-r12
+
+	 volatile floating point registers usable:
+	 f0-f13.  */
+
+FUNC_START (memcpy)
+	cmpld	cr0, r4, r3		/* if source==destination, return.  */
+	beqlr	cr0
+	/* if number of bytes is less than 8 but greater than zero,
+	   copy byte-by-byte.  */
+	cmpldi	r5, 8
+	mr	r6, r3
+	blt	L(copy_bytes)
+	neg	r0, r4
+	andi.	r11, r0, 7
+	beq	L(src_aligned)
+	/* We have to align the src pointer by r11 bytes */
+	cmplwi	cr1, r11, 4
+	cmplwi	cr0, r11, 1
+	bgt	cr1, L(src_567)
+	ble	cr0, L(src_1)
+	/* 2, 3 or 4 bytes */
+	addi	r0, r11, -2
+	lhz	r9, 0(r4)
+	lhzx	r12, r4, r0
+	sth	r9, 0(r6)
+	sthx	r12, r6, r0
+	b	L(src_0)
+L(src_567):
+	addi	r0, r11, -4
+	lwz	r9, 0(r4)
+	lwzx	r12, r4, r0
+	stw	r9, 0(r6)
+	stwx	r12, r6, r0
+	b	L(src_0)
+L(src_1):
+	lbz	r0, 0(r4)
+	stb	r0, 0(r6)
+L(src_0):
+	subf	r5, r11, r5
+	add	r4, r4, r11
+	add	r6, r6, r11
+L(src_aligned):
+	cmpldi	7, r5, 63
+	ble	7, L(copy_remaining)
+	srwi	r11, r5, 6		/* No of 64 byte copy count.  */
+	rlwinm.	r5, r5, 0, 26, 31	/* remaining bytes.  */
+	mtctr	r11
+L(copy_salign):
+	ld	r0, 0(r4)		/* 64-byte copy.  */
+	ld	r7, 8(r4)
+	ld	r8, 16(r4)
+	ld	r9, 24(r4)
+	std	r0, 0(r6)
+	std	r7, 8(r6)
+	std	r8, 16(r6)
+	std	r9, 24(r6)
+	ld	r0, 32(r4)
+	ld	r7, 40(r4)
+	ld	r8, 48(r4)
+	ld	r9, 56(r4)
+	addi	r4, r4, 64
+	std	r0, 32(r6)
+	std	r7, 40(r6)
+	std	r8, 48(r6)
+	std	r9, 56(r6)
+	addi	r6, r6, 64
+	bdnz	L(copy_salign)
+L(copy_remaining):
+	srwi.	r11, r5, 3		/* No of 8 byte copy count.  */
+	rlwinm	r5, r5, 0, 29, 31	/* remaining bytes.  */
+	beq	0, L(copy_bytes)
+	mtcrf	0x01, r11
+	bf	cr7*4+1, L(cp16b)
+	ld	r0, 0(r4)		/* copy 32 bytes.  */
+	ld	r7, 8(r4)
+	ld	r8, 16(r4)
+	ld	r9, 24(r4)
+	addi	r4, r4, 32
+	std	r0, 0(r6)
+	std	r7, 8(r6)
+	std	r8, 16(r6)
+	std	r9, 24(r6)
+	addi	r6, r6, 32
+L(cp16b):
+	bf	cr7*4+2, L(cp8b)
+	ld	r7, 0(r4)		/* copy 16 bytes.  */
+	ld	r8, 8(r4)
+	addi	r4, r4, 16
+	std	r7, 0(r6)
+	std	r8, 8(r6)
+	addi	r6, r6, 16
+L(cp8b):
+	bf	cr7*4+3, L(copy_bytes)
+	ld	r7, 0(r4)		/* copy 8 bytes.  */
+	addi	r4, r4, 8
+	std	r7, 0(r6)
+	addi	r6, r6, 8
+L(copy_bytes):
+	cmpldi	cr1, r5, 4
+	cmpldi	cr0, r5, 1
+	bgt	cr1, L(gt4b)		/* nb > 4?  (5, 6, 7 bytes).  */
+	ble	cr0, L(lt1b)		/* nb <= 1? (0, 1 bytes).  */
+	addi	r0, r5, -2		/* 2, 3, 4 bytes.  */
+	lhz	r9, 0(r4)
+	lhzx	r11, r4, r0
+	sth	r9, 0(r6)
+	sthx	r11, r6, r0
+	blr
+L(gt4b):
+	addi	r0, r5, -4		/* 5, 6, 7 bytes.  */
+	lwz	r9, 0(r4)
+	lwzx	r11, r4, r0
+	stw	r9, 0(r6)
+	stwx	r11, r6, r0
+	blr
+L(lt1b):
+	mtocrf	0x1, r5			/* nb == 0 ? return.  */
+	bflr	31
+	lbz	r0, 0(r4)		/* nb == 1.  */
+	stb	r0, 0(r6)
+	blr
+
+FUNC_END (memcpy)
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/e6500/32-bit/memcpy.S newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e6500/32-bit/memcpy.S
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/e6500/32-bit/memcpy.S	1969-12-31 18:00:00.000000000 -0600
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e6500/32-bit/memcpy.S	2015-09-25 19:06:11.196925090 -0500
@@ -0,0 +1,225 @@
+/* Optimized memcpy implementation for e6500 32-bit PowerPC.  */
+
+#include <powerpc/asm.h>
+
+/* _PTR [r3] memcpy 
+   (_PTR __restrict dst [r3], const _PTR __restrict src [r4], size_t len [r5]);
+   Returns 'dst'.
+
+	 r3 = destination
+	 r4 = source
+	 r5 = byte count
+
+	 volatile fixed point registers usable:
+	 r0, r3-r12
+
+	 volatile floating point registers usable:
+	 f0-f13.  */
+
+FUNC_START (memcpy)
+	cmplw	cr0, r4, r3		/* if source==destination, return.  */
+	beqlr	cr0
+	/* if number of bytes is less than 16, (optimal value TBD),
+	   but greater than zero copy byte-by-byte.  */
+	cmplwi	r5, 16
+	mr	r6, r3
+	blt	L(copy_remaining)
+	neg	r0, r3
+	andi.	r11, r0, 15
+	beq	L(dst_align16)
+	/* We have to align the src pointer by r11 bytes */
+	cmplwi	cr1, r11, 8
+	ble	cr1, L(src_1_8)
+	addi	r0, r11, -8
+	addi	r10, r11, -4
+	lwz	r9, 0(r4)
+	lwz	r8, 4(r4)
+	lwzx	r7, r4, r0
+	lwzx	r6, r4, r10
+	stw	r9, 0(r3)
+	stw	r8, 4(r3)
+	stwx	r7, r3, r0
+	stwx	r6, r3, r10
+	mr	r6, r3
+	b	L(src_0)
+L(src_1_8):
+	cmplwi	cr1, r11, 4
+	cmplwi	cr0, r11, 1
+	bgt	cr1, L(src_567)
+	ble	cr0, L(src_1)
+	/* 2, 3 or 4 bytes */
+	addi	r0, r11, -2
+	lhz	r9, 0(r4)
+	lhzx	r8, r4, r0
+	sth	r9, 0(r6)
+	sthx	r8, r6, r0
+	b	L(src_0)
+L(src_567):
+	addi	r0, r11, -4
+	lwz	r9, 0(r4)
+	lwzx	r8, r4, r0
+	stw	r9, 0(r6)
+	stwx	r8, r6, r0
+	b	L(src_0)
+L(src_1):
+	lbz	r0, 0(r4)
+	stb	r0, 0(r6)
+L(src_0):
+	subf	r5, r11, r5
+	add	r4, r4, r11
+	add	r6, r6, r11
+L(dst_align16):
+	cmplwi	7, r5, 63
+	ble	7, L(copy_remaining)
+	srwi	r11, r5, 6		/* No of 64 byte copy count.  */
+	rlwinm	r5, r5, 0, 26, 31	/* remaining bytes.  */
+	rlwinm.	r0, r4, 0, 28, 31
+	mtctr	r11
+	li	r7, 16
+	li	r8, 32
+	li	r9, 48
+	bne	0, L(src_naligned)
+L(copy_salign16):
+	lvx	v14, 0, r4		/* copy 64 bytes.  */
+	lvx	v15, r7, r4
+	lvx	v16, r8, r4
+	lvx	v17, r9, r4
+	addi	r4, r4, 64
+	stvx	v14, 0, r6
+	stvx	v15, r7, r6
+	stvx	v16, r8, r6
+	stvx	v17, r9, r6
+	addi	r6, r6, 64
+	bdnz	L(copy_salign16)
+L(copy_remaining):
+	srwi.	r11, r5, 3		/* No of 8 byte copy count.  */
+	rlwinm	r5, r5, 0, 29, 31	/* remaining bytes.  */
+	beq	0, L(copy_bytes)
+	mtcrf	0x01, r11
+	bf	cr7*4+1, L(cp16b)
+
+	lwz	r0, 0(r4)		/* copy 32 bytes */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+
+	lwz	r0, 16(r4)
+	lwz	r7, 20(r4)
+	lwz	r8, 24(r4)
+	lwz	r9, 28(r4)
+	addi	r4, r4, 32
+
+	stw	r0, 16(r6)
+	stw	r7, 20(r6)
+	stw	r8, 24(r6)
+	stw	r9, 28(r6)
+	addi r6, r6, 32
+L(cp16b):
+	bf	cr7*4+2, L(cp8b)
+	lwz	r0, 0(r4)		/* copy 16 bytes */
+	lwz	r7, 4(r4)
+	lwz	r8, 8(r4)
+	lwz	r9, 12(r4)
+
+	addi	r4, r4, 16
+
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	stw	r8, 8(r6)
+	stw	r9, 12(r6)
+	addi	r6, r6, 16
+L(cp8b):
+	bf	cr7*4+3, L(copy_bytes)
+	lwz	r0, 0(r4)		/* copy 8 bytes */
+	lwz	r7, 4(r4)
+	addi	r4, r4, 8
+
+	stw	r0, 0(r6)
+	stw	r7, 4(r6)
+	addi	r6, r6, 8
+L(copy_bytes):
+	cmplwi	cr1, r5, 4
+	cmplwi	cr0, r5, 1
+	bgt	cr1, L(gt4b)		/* nb > 4?  (5, 6, 7 bytes).  */
+	ble	cr0, L(lt1b)		/* nb <= 1? (0, 1 bytes).  */
+	addi	r0, r5, -2		/* 2, 3, 4 bytes.  */
+	lhz	r9, 0(r4)
+	lhzx	r11, r4, r0
+	sth	r9, 0(r6)
+	sthx	r11, r6, r0
+	blr
+L(gt4b):
+	addi	r0, r5, -4		/* 5, 6, 7 bytes.  */
+	lwz	r9, 0(r4)
+	lwzx	r11, r4, r0
+	stw	r9, 0(r6)
+	stwx	r11, r6, r0
+	blr
+L(lt1b):
+	mtocrf	0x1, r5			/* nb == 0 ? return.  */
+	bflr	31
+	lbz	r0, 0(r4)		/* nb == 1.  */
+	stb	r0, 0(r6)
+	blr
+
+L(src_naligned):
+#ifndef _SOFT_FLOAT
+	rlwinm.	r0, r4, 0, 29, 31
+	beq	0, L(copy_salign8)
+#endif
+L(copy_snalign):			/* copy 64 bytes.  */
+	lvx	v0, 0, r4		/* load MSQ.  */
+	lvsl	v18, 0, r4		/* set permute control vector.  */
+	lvx	v19, r7, r4		/* load LSQ.  */
+	vperm	v14, v0, v19, v18	/* align the data.  */
+	lvx	v0, r7, r4		/* load MSQ.  */
+	lvsl	v18, r7, r4		/* set permute control vector.  */
+	lvx	v19, r8, r4		/* load LSQ.  */
+	vperm	v15, v0, v19, v18	/* align the data.  */
+	lvx	v0, r8, r4		/* load MSQ.  */
+	lvsl	v18, r8, r4		/* set permute control vector.  */
+	lvx	v19, r9, r4		/* load LSQ.  */
+	vperm	v16, v0, v19, v18	/* align the data.  */
+	lvx	v0, r9, r4		/* load MSQ.  */
+	lvsl	v18, r9, r4		/* set permute control vector.  */
+	addi	r4, r4, 64
+	lvx	v19, 0, r4		/* load LSQ.  */
+	vperm	v17, v0, v19, v18	/* align the data.  */
+	stvx	v14, 0, r6
+	stvx	v15, r7, r6
+	stvx	v16, r8, r6
+	stvx	v17, r9, r6
+	addi	r6, r6, 64
+	bdnz	L(copy_snalign)
+	b	L(copy_remaining)
+
+#ifndef _SOFT_FLOAT
+L(copy_salign8):
+	lfd	0, 0(r4)		/* copy 64 bytes.  */
+	lfd	1, 8(r4)
+	lfd	2, 16(r4)
+	lfd	3, 24(r4)
+	stfd	0, 0(r6)
+	stfd	1, 8(r6)
+	stfd	2, 16(r6)
+	stfd	3, 24(r6)
+	lfd	0, 32(r4)
+	lfd	1, 40(r4)
+	lfd	2, 48(r4)
+	lfd	3, 56(r4)
+	addi	r4, r4, 64
+	stfd	0, 32(r6)
+	stfd	1, 40(r6)
+	stfd	2, 48(r6)
+	stfd	3, 56(r6)
+	addi	r6, r6, 64
+	bdnz	L(copy_salign8)
+	b	L(copy_remaining)
+#endif
+
+FUNC_END (memcpy)
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/e6500/64-bit/memcpy.S newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e6500/64-bit/memcpy.S
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/e6500/64-bit/memcpy.S	1969-12-31 18:00:00.000000000 -0600
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/e6500/64-bit/memcpy.S	2015-09-25 19:06:24.510925286 -0500
@@ -0,0 +1,194 @@
+/* Optimized memcpy implementation for e6500 64-bit PowerPC.  */
+
+#include <powerpc/asm.h>
+
+/* _PTR [r3] memcpy 
+   (_PTR __restrict dst [r3], const _PTR __restrict src [r4], size_t len [r5]);
+   Returns 'dst'.
+
+	 r3 = destination
+	 r4 = source
+	 r5 = byte count
+
+	 volatile fixed point registers usable:
+	 r0, r3-r12
+
+	 volatile floating point registers usable:
+	 f0-f13.  */
+
+FUNC_START (memcpy)
+	cmpld	cr0, r4, r3		/* if source==destination, return.  */
+	beqlr	cr0
+	/* if number of bytes is less than 16 but greater than zero,
+	   copy byte-by-byte.  */
+	cmpldi	r5, 16
+	mr	r6, r3
+	ble	L(copy_remaining)
+	neg	r0, r3
+	andi.	r11, r0, 15
+	beq	L(dst_align)
+ 	/* We have to align the src pointer by r11 bytes */
+	cmplwi	cr1, r11, 8
+	ble	cr1, L(src_1_8)
+	addi	r0, r11, -8
+	ld	r9, 0(r4)
+	ldx	r8, r4, r0
+	std	r9, 0(r6)
+	stdx	r8, r6, r0
+	b	L(src_0)
+L(src_1_8):
+	cmplwi	cr1, r11, 4
+	cmplwi	cr0, r11, 1
+	bgt	cr1, L(src_567)
+	ble	cr0, L(src_1)
+	/* 2, 3 or 4 bytes */
+	addi	r0, r11, -2
+	lhz	r9, 0(r4)
+	lhzx	r8, r4, r0
+	sth	r9, 0(r6)
+	sthx	r8, r6, r0
+	b	L(src_0)
+L(src_567):
+	addi	r0, r11, -4
+	lwz	r9, 0(r4)
+	lwzx	r8, r4, r0
+	stw	r9, 0(r6)
+	stwx	r8, r6, r0
+	b	L(src_0)
+L(src_1):
+	lbz	r0, 0(r4)
+	stb	r0, 0(r6)
+L(src_0):
+	subf	r5, r11, r5
+	add	r4, r4, r11
+	add	r6, r6, r11
+L(dst_align):
+	cmpldi	7, r5, 63
+	ble	7, L(copy_remaining)
+	srwi	r11, r5, 6		/* No of 64 byte copy count.  */
+	rlwinm	r5, r5, 0, 26, 31	/* remaining bytes.  */
+	rlwinm.	r0, r4, 0, 28, 31
+	mtctr	r11
+	li	r7, 16
+	li	r8, 32
+	li	r9, 48
+	bne	0, L(src_naligned)
+L(copy_salign):
+	lvx	v14, 0, r4
+	lvx	v15, r7, r4
+	lvx	v16, r8, r4
+	lvx	v17, r9, r4
+	addi	r4, r4, 64
+	stvx	v14, 0, r6
+	stvx	v15, r7, r6
+	stvx	v16, r8, r6
+	stvx	v17, r9, r6
+	addi	r6, r6, 64
+	bdnz	L(copy_salign)
+L(copy_remaining):
+	srwi.	r11, r5, 3		/* No of 8 byte copy count.  */
+	rlwinm	r5, r5, 0, 29, 31	/* remaining bytes.  */
+	beq	0, L(copy_bytes)
+	mtcrf	0x01, r11
+	bf	cr7*4+1, L(cp16b)
+	ld	r0, 0(r4)		/* copy 32 bytes.  */
+	ld	r7, 8(r4)
+	ld	r8, 16(r4)
+	ld	r9, 24(r4)
+	addi	r4, r4, 32
+	std	r0, 0(r6)
+	std	r7, 8(r6)
+	std	r8, 16(r6)
+	std	r9, 24(r6)
+	addi	r6, r6, 32
+L(cp16b):
+	bf	cr7*4+2, L(cp8b)
+	ld	r7, 0(r4)		/* copy 16 bytes.  */
+	ld	r8, 8(r4)
+	addi	r4, r4, 16
+	std	r7, 0(r6)
+	std	r8, 8(r6)
+	addi	r6, r6, 16
+L(cp8b):
+	bf	cr7*4+3, L(copy_bytes)
+	ld	r7, 0(r4)		/* copy 8 bytes.  */
+	addi	r4, r4, 8
+	std	r7, 0(r6)
+	addi	r6, r6, 8
+L(copy_bytes):
+	cmpldi	cr1, r5, 4
+	cmpldi	cr0, r5, 1
+	bgt	cr1, L(gt4b)		/* nb > 4?  (5, 6, 7 bytes).  */
+	ble	cr0, L(lt1b)		/* nb <= 1? (0, 1 bytes).  */
+	addi	r0, r5, -2		/* 2, 3, 4 bytes.  */
+	lhz	r9, 0(r4)
+	lhzx	r11, r4, r0
+	sth	r9, 0(r6)
+	sthx	r11, r6, r0
+	blr
+L(gt4b):
+	addi	r0, r5, -4		/* 5, 6, 7 bytes.  */
+	lwz	r9, 0(r4)
+	lwzx	r11, r4, r0
+	stw	r9, 0(r6)
+	stwx	r11, r6, r0
+	blr
+L(lt1b):
+	mtocrf	0x1, r5			/* nb == 0 ? return.  */
+	bflr	31
+	lbz	r0, 0(r4)		/* nb == 1.  */
+	stb	r0, 0(r6)
+	blr
+
+L(src_naligned):
+	rlwinm.	r0, r4, 0, 29, 31
+	beq	0, L(copy_salign8)
+L(copy_snalign):
+	lvx	v0, 0, r4		/* load MSQ.  */
+	lvsl	v18, 0, r4		/* set permute control vector.  */
+	lvx	v19, r7, r4		/* load LSQ.  */
+	vperm	v14, v0, v19, v18	/* align the data.  */
+	lvx	v0, r7, r4		/* load MSQ.  */
+	lvsl	v18, r7, r4		/* set permute control vector.  */
+	lvx	v19, r8, r4		/* load LSQ.  */
+	vperm	v15, v0, v19, v18	/* align the data.  */
+	lvx	v0, r8, r4		/* load MSQ.  */
+	lvsl	v18, r8, r4		/* set permute control vector.  */
+	lvx	v19, r9, r4		/* load LSQ.  */
+	vperm	v16, v0, v19, v18	/* align the data.  */
+	lvx	v0, r9, r4		/* load MSQ.  */
+	lvsl	v18, r9, r4		/* set permute control vector.  */
+	addi	r4, r4, 64
+	lvx	v19, 0, r4		/* load LSQ.  */
+	vperm	v17, v0, v19, v18	/* align the data.  */
+	stvx	v14, 0, r6
+	stvx	v15, r7, r6
+	stvx	v16, r8, r6
+	stvx	v17, r9, r6
+	addi	r6, r6, 64
+	bdnz	L(copy_snalign)
+	b	L(copy_remaining)
+
+L(copy_salign8):
+	ld	r0, 0(r4)
+	ld	r7, 8(r4)
+	ld	r8, 16(r4)
+	ld	r9, 24(r4)
+	std	r0, 0(r6)
+	std	r7, 8(r6)
+	std	r8, 16(r6)
+	std	r9, 24(r6)
+	ld	r0, 32(r4)
+	ld	r7, 40(r4)
+	ld	r8, 48(r4)
+	ld	r9, 56(r4)
+	addi	r4, r4, 64
+	std	r0, 32(r6)
+	std	r7, 40(r6)
+	std	r8, 48(r6)
+	std	r9, 56(r6)
+	addi	r6, r6, 64
+	bdnz	L(copy_salign8)
+	b	L(copy_remaining)
+
+FUNC_END (memcpy)
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/Makefile.am newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/Makefile.am
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/Makefile.am	2015-09-23 14:29:36.316121000 -0500
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/Makefile.am	2015-09-23 14:29:04.120121000 -0500
@@ -1,16 +1,39 @@
 ## Process this file with automake to generate Makefile.in
 
-AUTOMAKE_OPTIONS = cygnus
+AUTOMAKE_OPTIONS = cygnus subdir-objects
 
 INCLUDES = $(NEWLIB_CFLAGS) $(CROSS_CFLAGS) $(TARGET_CFLAGS)
 
-AM_CCASFLAGS = $(INCLUDES)
+AM_CCASFLAGS = $(INCLUDES) -D__ASSEMBLER__
 
 noinst_LIBRARIES = lib.a
 
 AM_CFLAGS = -I $(srcdir)/../../stdio -I $(srcdir)/../../stdlib
 
 lib_a_SOURCES = setjmp.S
+
+if E500MC
+lib_a_SOURCES += e500mc/memcpy.S
+else		#E500MC else
+if E5500
+if TARGET_64
+lib_a_SOURCES += e5500/64-bit/memcpy.S
+else		#E5500 TARGET_64 else
+lib_a_SOURCES += e5500/32-bit/memcpy.S
+endif		#E5500 TARGET_64
+else		#E5500 else
+if E6500
+if TARGET_64
+lib_a_SOURCES += e6500/64-bit/memcpy.S
+else		#E6500 TARGET_64 else
+lib_a_SOURCES += e6500/32-bit/memcpy.S
+endif		#E6500 TARGET_64
+else            #e6500 else
+lib_a_SOURCES += memcpy.c
+endif		#E6500
+endif		#E5500
+endif		#E500MC
+
 lib_a_CCASFLAGS=$(AM_CCASFLAGS)
 lib_a_CFLAGS=$(AM_CFLAGS)
 EXTRA_lib_a_SOURCES = \
diff -ruN newlib-2.2.0-orig/newlib/libc/machine/powerpc/Makefile.in newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/Makefile.in
--- newlib-2.2.0-orig/newlib/libc/machine/powerpc/Makefile.in	2015-09-23 14:29:36.316121000 -0500
+++ newlib-2.2.0-memcpy-fixed/newlib/libc/machine/powerpc/Makefile.in	2015-09-23 14:29:04.120121000 -0500
@@ -51,6 +51,12 @@
 POST_UNINSTALL = :
 build_triplet = @build@
 host_triplet = @host@
+@E500MC_TRUE@am__append_1 = e500mc/memcpy.S
+@E500MC_FALSE@@E5500_TRUE@@TARGET_64_TRUE@am__append_2 = e5500/64-bit/memcpy.S
+@E500MC_FALSE@@E5500_TRUE@@TARGET_64_FALSE@am__append_3 = e5500/32-bit/memcpy.S
+@E500MC_FALSE@@E5500_FALSE@@E6500_TRUE@@TARGET_64_TRUE@am__append_4 = e6500/64-bit/memcpy.S
+@E500MC_FALSE@@E5500_FALSE@@E6500_TRUE@@TARGET_64_FALSE@am__append_5 = e6500/32-bit/memcpy.S
+@E500MC_FALSE@@E5500_FALSE@@E6500_FALSE@am__append_6 = memcpy.c
 subdir = .
 DIST_COMMON = $(srcdir)/Makefile.in $(srcdir)/Makefile.am \
 	$(top_srcdir)/configure $(am__configure_deps) \
@@ -69,7 +75,16 @@
 ARFLAGS = cru
 lib_a_AR = $(AR) $(ARFLAGS)
 lib_a_LIBADD =
-am_lib_a_OBJECTS = lib_a-setjmp.$(OBJEXT)
+am__dirstamp = $(am__leading_dot)dirstamp
+@E500MC_TRUE@am__objects_1 = e500mc/lib_a-memcpy.$(OBJEXT)
+@E500MC_FALSE@@E5500_TRUE@@TARGET_64_TRUE@am__objects_2 = e5500/64-bit/lib_a-memcpy.$(OBJEXT)
+@E500MC_FALSE@@E5500_TRUE@@TARGET_64_FALSE@am__objects_3 = e5500/32-bit/lib_a-memcpy.$(OBJEXT)
+@E500MC_FALSE@@E5500_FALSE@@E6500_TRUE@@TARGET_64_TRUE@am__objects_4 = e6500/64-bit/lib_a-memcpy.$(OBJEXT)
+@E500MC_FALSE@@E5500_FALSE@@E6500_TRUE@@TARGET_64_FALSE@am__objects_5 = e6500/32-bit/lib_a-memcpy.$(OBJEXT)
+@E500MC_FALSE@@E5500_FALSE@@E6500_FALSE@am__objects_6 = lib_a-memcpy.$(OBJEXT)
+am_lib_a_OBJECTS = lib_a-setjmp.$(OBJEXT) \
+	$(am__objects_1) $(am__objects_2) $(am__objects_3) \
+	$(am__objects_4) $(am__objects_5) $(am__objects_6)
 lib_a_OBJECTS = $(am_lib_a_OBJECTS)
 DEFAULT_INCLUDES = -I.@am__isrc@
 depcomp =
@@ -192,12 +207,14 @@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
-AUTOMAKE_OPTIONS = cygnus
+AUTOMAKE_OPTIONS = cygnus subdir-objects
 INCLUDES = $(NEWLIB_CFLAGS) $(CROSS_CFLAGS) $(TARGET_CFLAGS)
-AM_CCASFLAGS = $(INCLUDES)
+AM_CCASFLAGS = $(INCLUDES) -D__ASSEMBLER__
 noinst_LIBRARIES = lib.a
 AM_CFLAGS = -I $(srcdir)/../../stdio -I $(srcdir)/../../stdlib
-lib_a_SOURCES = setjmp.S
+lib_a_SOURCES = setjmp.S $(am__append_1) $(am__append_2) \
+	$(am__append_3) $(am__append_4) $(am__append_5) \
+	$(am__append_6)
 lib_a_CCASFLAGS = $(AM_CCASFLAGS)
 lib_a_CFLAGS = $(AM_CFLAGS)
 EXTRA_lib_a_SOURCES = \
@@ -250,6 +267,26 @@
 
 clean-noinstLIBRARIES:
 	-test -z "$(noinst_LIBRARIES)" || rm -f $(noinst_LIBRARIES)
+e500mc/$(am__dirstamp):
+	@$(MKDIR_P) e500mc
+	@: > e500mc/$(am__dirstamp)
+e500mc/lib_a-memcpy.$(OBJEXT): e500mc/$(am__dirstamp)
+e5500/64-bit/$(am__dirstamp):
+	@$(MKDIR_P) e5500/64-bit
+	@: > e5500/64-bit/$(am__dirstamp)
+e5500/64-bit/lib_a-memcpy.$(OBJEXT): e5500/64-bit/$(am__dirstamp)
+e5500/32-bit/$(am__dirstamp):
+	@$(MKDIR_P) e5500/32-bit
+	@: > e5500/32-bit/$(am__dirstamp)
+e5500/32-bit/lib_a-memcpy.$(OBJEXT): e5500/32-bit/$(am__dirstamp)
+e6500/64-bit/$(am__dirstamp):
+	@$(MKDIR_P) e6500/64-bit
+	@: > e6500/64-bit/$(am__dirstamp)
+e6500/64-bit/lib_a-memcpy.$(OBJEXT): e6500/64-bit/$(am__dirstamp)
+e6500/32-bit/$(am__dirstamp):
+	@$(MKDIR_P) e6500/32-bit
+	@: > e6500/32-bit/$(am__dirstamp)
+e6500/32-bit/lib_a-memcpy.$(OBJEXT): e6500/32-bit/$(am__dirstamp)
 lib.a: $(lib_a_OBJECTS) $(lib_a_DEPENDENCIES) $(EXTRA_lib_a_DEPENDENCIES) 
 	-rm -f lib.a
 	$(lib_a_AR) lib.a $(lib_a_OBJECTS) $(lib_a_LIBADD)
@@ -257,6 +294,11 @@
 
 mostlyclean-compile:
 	-rm -f *.$(OBJEXT)
+	-rm -f e500mc/lib_a-memcpy.$(OBJEXT)
+	-rm -f e5500/32-bit/lib_a-memcpy.$(OBJEXT)
+	-rm -f e5500/64-bit/lib_a-memcpy.$(OBJEXT)
+	-rm -f e6500/32-bit/lib_a-memcpy.$(OBJEXT)
+	-rm -f e6500/64-bit/lib_a-memcpy.$(OBJEXT)
 
 distclean-compile:
 	-rm -f *.tab.c
@@ -273,11 +315,41 @@
 lib_a-setjmp.obj: setjmp.S
 	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o lib_a-setjmp.obj `if test -f 'setjmp.S'; then $(CYGPATH_W) 'setjmp.S'; else $(CYGPATH_W) '$(srcdir)/setjmp.S'; fi`
 
+e500mc/lib_a-memcpy.o: e500mc/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e500mc/lib_a-memcpy.o `test -f 'e500mc/memcpy.S' || echo '$(srcdir)/'`e500mc/memcpy.S
+
+e500mc/lib_a-memcpy.obj: e500mc/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e500mc/lib_a-memcpy.obj `if test -f 'e500mc/memcpy.S'; then $(CYGPATH_W) 'e500mc/memcpy.S'; else $(CYGPATH_W) '$(srcdir)/e500mc/memcpy.S'; fi`
+
+e5500/64-bit/lib_a-memcpy.o: e5500/64-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e5500/64-bit/lib_a-memcpy.o `test -f 'e5500/64-bit/memcpy.S' || echo '$(srcdir)/'`e5500/64-bit/memcpy.S
+
+e5500/64-bit/lib_a-memcpy.obj: e5500/64-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e5500/64-bit/lib_a-memcpy.obj `if test -f 'e5500/64-bit/memcpy.S'; then $(CYGPATH_W) 'e5500/64-bit/memcpy.S'; else $(CYGPATH_W) '$(srcdir)/e5500/64-bit/memcpy.S'; fi`
+
+e5500/32-bit/lib_a-memcpy.o: e5500/32-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e5500/32-bit/lib_a-memcpy.o `test -f 'e5500/32-bit/memcpy.S' || echo '$(srcdir)/'`e5500/32-bit/memcpy.S
+
+e5500/32-bit/lib_a-memcpy.obj: e5500/32-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e5500/32-bit/lib_a-memcpy.obj `if test -f 'e5500/32-bit/memcpy.S'; then $(CYGPATH_W) 'e5500/32-bit/memcpy.S'; else $(CYGPATH_W) '$(srcdir)/e5500/32-bit/memcpy.S'; fi`
+
+e6500/64-bit/lib_a-memcpy.o: e6500/64-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e6500/64-bit/lib_a-memcpy.o `test -f 'e6500/64-bit/memcpy.S' || echo '$(srcdir)/'`e6500/64-bit/memcpy.S
+
+e6500/64-bit/lib_a-memcpy.obj: e6500/64-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e6500/64-bit/lib_a-memcpy.obj `if test -f 'e6500/64-bit/memcpy.S'; then $(CYGPATH_W) 'e6500/64-bit/memcpy.S'; else $(CYGPATH_W) '$(srcdir)/e6500/64-bit/memcpy.S'; fi`
+
+e6500/32-bit/lib_a-memcpy.o: e6500/32-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e6500/32-bit/lib_a-memcpy.o `test -f 'e6500/32-bit/memcpy.S' || echo '$(srcdir)/'`e6500/32-bit/memcpy.S
+
+e6500/32-bit/lib_a-memcpy.obj: e6500/32-bit/memcpy.S
+	$(CCAS) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CCASFLAGS) $(CCASFLAGS) -c -o e6500/32-bit/lib_a-memcpy.obj `if test -f 'e6500/32-bit/memcpy.S'; then $(CYGPATH_W) 'e6500/32-bit/memcpy.S'; else $(CYGPATH_W) '$(srcdir)/e6500/32-bit/memcpy.S'; fi`
+
 .c.o:
-	$(COMPILE) -c $<
+	$(COMPILE) -c -o $@ $<
 
 .c.obj:
-	$(COMPILE) -c `$(CYGPATH_W) '$<'`
+	$(COMPILE) -c -o $@ `$(CYGPATH_W) '$<'`
 
 lib_a-vfprintf.o: vfprintf.c
 	$(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CFLAGS) $(CFLAGS) -c -o lib_a-vfprintf.o `test -f 'vfprintf.c' || echo '$(srcdir)/'`vfprintf.c
@@ -405,6 +477,12 @@
 lib_a-ufix64toa.obj: ufix64toa.c
 	$(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CFLAGS) $(CFLAGS) -c -o lib_a-ufix64toa.obj `if test -f 'ufix64toa.c'; then $(CYGPATH_W) 'ufix64toa.c'; else $(CYGPATH_W) '$(srcdir)/ufix64toa.c'; fi`
 
+lib_a-memcpy.o: memcpy.c
+	$(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CFLAGS) $(CFLAGS) -c -o lib_a-memcpy.o `test -f 'memcpy.c' || echo '$(srcdir)/'`memcpy.c
+
+lib_a-memcpy.obj: memcpy.c
+	$(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(lib_a_CFLAGS) $(CFLAGS) -c -o lib_a-memcpy.obj `if test -f 'memcpy.c'; then $(CYGPATH_W) 'memcpy.c'; else $(CYGPATH_W) '$(srcdir)/memcpy.c'; fi`
+
 ID: $(HEADERS) $(SOURCES) $(LISP) $(TAGS_FILES)
 	list='$(SOURCES) $(HEADERS) $(LISP) $(TAGS_FILES)'; \
 	unique=`for i in $$list; do \
@@ -486,6 +564,11 @@
 distclean-generic:
 	-test -z "$(CONFIG_CLEAN_FILES)" || rm -f $(CONFIG_CLEAN_FILES)
 	-test . = "$(srcdir)" || test -z "$(CONFIG_CLEAN_VPATH_FILES)" || rm -f $(CONFIG_CLEAN_VPATH_FILES)
+	-rm -f e500mc/$(am__dirstamp)
+	-rm -f e5500/32-bit/$(am__dirstamp)
+	-rm -f e5500/64-bit/$(am__dirstamp)
+	-rm -f e6500/32-bit/$(am__dirstamp)
+	-rm -f e6500/64-bit/$(am__dirstamp)
 
 maintainer-clean-generic:
 	@echo "This command is intended for maintainers to use"
